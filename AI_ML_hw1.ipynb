{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmBbRUKBcfntE2/JZjkfxd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ANS1514/Projects_in_AI-ML/blob/main/AI_ML_hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The objective function for logistic regression using MLE is the log-likeliood function or the negative log-likelihood (NLL) function, usaully preferring the NLL due to the focus on minimizing rather than maximizing. MAP, Maximum a posteriori, is a technique that is based on Bayes' theorem, it looks to find parameters that maximixe posterior probablility. An example give in researching MAP was MLE measures an NBA players free throw percentage from game to game, while MAP accounts for their career free throw percentage. MLE performs very well with large datasets as its ability to estimate gets better, while MAP helps more to regularize, reduce overitting, and incorperate prior data resulting in a more balanced estimation.\n",
        "\n",
        "$$\n",
        "NLL = -\\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^C y_{ij} log(p_{ij})\n",
        "$$\n",
        "\n",
        "2. A problem I would solve using Logistic Regression would be to predict if a customer would buy a product. Ever since I learned that Netflix's recommendation algorithm was based on machine learning I wanted to learn exactly how and try for myself, and I feel you can apply this concept to purchasing products as well. A comsumer will either buy a product or won't so it's a classification problem, logistic regression would be best for this case due to it's ability to predict based on linearly seperable data and account for previous decisions.\n",
        "\n",
        "3. How my dataset would compare to the variables in my equations, n = # data points, c = # classes, y_ij = binary indicator (0 or 1), p_ij = predicted probability that i belongs to j. We assume that the data is i.i.d, the classes are 'buy', 'not buy', and there is a linear relationship between the features.\n",
        "\n",
        "\n",
        "\n",
        "Citation\n",
        "https://medium.com/@devcharlie2698619/the-intuition-behind-maximum-likelihood-estimation-mle-and-maximum-a-posteriori-estimation-map-b8ba1ba1078f\n",
        "https://medium.com/aimonks/loss-function-the-secret-ingredient-to-building-high-performance-ai-models-88415112b94d\n",
        "\n"
      ],
      "metadata": {
        "id": "t6iewJm2tBJI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tu_TDW4EFLl3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61071875-f262-404e-d70e-cbcb70bc0ceb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e7237d97-093b-44c9-8409-69b4d2fe47f7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e7237d97-093b-44c9-8409-69b4d2fe47f7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving digital_marketing_campaign_dataset.csv to digital_marketing_campaign_dataset.csv\n",
            "      CustomerID  Age  Gender  Income CampaignChannel   CampaignType  \\\n",
            "0           8000   56  Female  136912    Social Media      Awareness   \n",
            "1           8001   69    Male   41760           Email      Retention   \n",
            "2           8002   46  Female   88456             PPC      Awareness   \n",
            "3           8003   32  Female   44085             PPC     Conversion   \n",
            "4           8004   60  Female   83964             PPC     Conversion   \n",
            "...          ...  ...     ...     ...             ...            ...   \n",
            "7995       15995   21    Male   24849           Email      Awareness   \n",
            "7996       15996   43  Female   44718             SEO      Retention   \n",
            "7997       15997   28  Female  125471        Referral  Consideration   \n",
            "7998       15998   19  Female  107862             PPC  Consideration   \n",
            "7999       15999   31  Female   93002           Email      Awareness   \n",
            "\n",
            "          AdSpend  ClickThroughRate  ConversionRate  WebsiteVisits  \\\n",
            "0     6497.870068          0.043919        0.088031              0   \n",
            "1     3898.668606          0.155725        0.182725             42   \n",
            "2     1546.429596          0.277490        0.076423              2   \n",
            "3      539.525936          0.137611        0.088004             47   \n",
            "4     1678.043573          0.252851        0.109940              0   \n",
            "...           ...               ...             ...            ...   \n",
            "7995  8518.308575          0.243792        0.116773             23   \n",
            "7996  1424.613446          0.236740        0.190061             49   \n",
            "7997  4609.534635          0.056526        0.133826             35   \n",
            "7998  9476.106354          0.023961        0.138386             49   \n",
            "7999  7743.627070          0.185670        0.057228             15   \n",
            "\n",
            "      PagesPerVisit  TimeOnSite  SocialShares  EmailOpens  EmailClicks  \\\n",
            "0          2.399017    7.396803            19           6            9   \n",
            "1          2.917138    5.352549             5           2            7   \n",
            "2          8.223619   13.794901             0          11            2   \n",
            "3          4.540939   14.688363            89           2            2   \n",
            "4          2.046847   13.993370             6           6            6   \n",
            "...             ...         ...           ...         ...          ...   \n",
            "7995       9.693602   14.227794            70          13            6   \n",
            "7996       9.499010    3.501106            52          13            1   \n",
            "7997       2.853241   14.618323            38          16            0   \n",
            "7998       1.002964    3.876623            86           1            5   \n",
            "7999       6.964739   12.763660             2          18            9   \n",
            "\n",
            "      PreviousPurchases  LoyaltyPoints AdvertisingPlatform AdvertisingTool  \\\n",
            "0                     4            688            IsConfid      ToolConfid   \n",
            "1                     2           3459            IsConfid      ToolConfid   \n",
            "2                     8           2337            IsConfid      ToolConfid   \n",
            "3                     0           2463            IsConfid      ToolConfid   \n",
            "4                     8           4345            IsConfid      ToolConfid   \n",
            "...                 ...            ...                 ...             ...   \n",
            "7995                  7            286            IsConfid      ToolConfid   \n",
            "7996                  5           1502            IsConfid      ToolConfid   \n",
            "7997                  3            738            IsConfid      ToolConfid   \n",
            "7998                  7           2709            IsConfid      ToolConfid   \n",
            "7999                  9            341            IsConfid      ToolConfid   \n",
            "\n",
            "      Conversion  \n",
            "0              1  \n",
            "1              1  \n",
            "2              1  \n",
            "3              1  \n",
            "4              1  \n",
            "...          ...  \n",
            "7995           0  \n",
            "7996           0  \n",
            "7997           1  \n",
            "7998           1  \n",
            "7999           0  \n",
            "\n",
            "[8000 rows x 20 columns]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from google.colab import files\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid, ParameterSampler\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Upload the file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# File is on the github repo\n",
        "df = pd.read_csv(\"digital_marketing_campaign_dataset.csv\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial Findings\n",
        "df.info()\n",
        "print(\"\\nUnique Values\\n\", df.nunique())\n",
        "print(df)"
      ],
      "metadata": {
        "id": "pAwqsJ5ktFN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns with 1 unique value\n",
        "df.drop(columns=['AdvertisingPlatform', 'AdvertisingTool'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "QDDmOxXKtGkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = preprocessing.LabelEncoder()\n",
        "df_encoded = df\n",
        "df_encoded['Gender'] = label_encoder.fit_transform(df_encoded['Gender'])\n",
        "df_encoded['CampaignChannel'] = label_encoder.fit_transform(df_encoded['CampaignChannel'])\n",
        "df_encoded['CampaignType'] = label_encoder.fit_transform(df_encoded['CampaignType'])\n",
        "corr_matrix = df_encoded.corr()\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='ocean', vmin=-1, vmax=1)\n",
        "plt.title(\"Feature Correlation\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wtArMZGetH4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vif_data = pd.DataFrame()\n",
        "vif_data['Feature'] = df_encoded.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(df_encoded.values, i) for i in range(len(df_encoded.columns))]\n",
        "\n",
        "print(vif_data)\n",
        "# We can keep all of the other features since their VIF is greater than 1"
      ],
      "metadata": {
        "id": "ZGPPDqXrtNBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = df_encoded.corr()['Conversion'].sort_values()\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "plt.title(\"Correlation coefficient for Conversions\")\n",
        "sns.barplot(x=corr_matrix.index.values.tolist(), y=corr_matrix.values.tolist(), palette='ocean')"
      ],
      "metadata": {
        "id": "owWrXfR2tRC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def compute_loss(y_target, y_pred, epsilon):\n",
        "  # binary cross entropy\n",
        "  y1 = y_target * np.log(y_pred + epsilon)\n",
        "  y2 = (1-y_target) * np.log(1 - y_pred + epsilon)\n",
        "  return -np.mean(y1 + y2)\n",
        "\n",
        "def gradient(x, y, weights, b):\n",
        "  a = sigmoid(np.dot(x, weights) + b)\n",
        "  error = a - y\n",
        "  dw = np.dot(error, x) / len(y)\n",
        "  db = np.mean(error)\n",
        "  return dw, db\n",
        "\n",
        "def get_ypred(X, w, b):\n",
        "  p = sigmoid(np.dot(X, w) + b)\n",
        "  p_score = p.copy()\n",
        "  p[p > 0.5] = 1\n",
        "  p[p <= 0.5] = 0\n",
        "  return p, p_score\n",
        "\n",
        "def evaluate_metrics(y_pred, y_true):\n",
        "  accuracy = accuracy_score(y_true, y_pred)\n",
        "  precision = precision_score(y_true, y_pred)\n",
        "  recall = recall_score(y_true, y_pred)\n",
        "  f1 = f1_score(y_true, y_pred)\n",
        "  return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
      ],
      "metadata": {
        "id": "g4tXE1DPtS5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizer:\n",
        "    def __init__(self, weights, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-9):\n",
        "        self.weights = weights\n",
        "        self.lr = lr\n",
        "        self.momentum = np.zeros_like(weights)\n",
        "        self.squared_grad = np.zeros_like(weights)\n",
        "        self.beta1 = beta1  # For Momentum/Adam\n",
        "        self.beta2 = beta2  # For RMSProp/Adam\n",
        "        self.t = 0  # Time step (for Adam)\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def momentum_update(self, dw):\n",
        "        self.momentum = self.beta1 * self.momentum + (1 - self.beta1) * dw\n",
        "        self.weights -= self.lr * self.momentum\n",
        "\n",
        "    def rmsprop_update(self, dw):\n",
        "        self.squared_grad = self.beta2 * self.squared_grad + (1 - self.beta2) * (dw**2)\n",
        "        self.weights -= self.lr * dw / (np.sqrt(self.squared_grad) + self.epsilon)\n",
        "\n",
        "    def adam_update(self, dw):\n",
        "        self.t += 1\n",
        "        self.momentum = self.beta1 * self.momentum + (1 - self.beta1) * dw\n",
        "        self.squared_grad = self.beta2 * self.squared_grad + (1 - self.beta2) * (dw**2)\n",
        "        m_hat = self.momentum / (1 - self.beta1**self.t)\n",
        "        v_hat = self.squared_grad / (1 - self.beta2**self.t)\n",
        "        self.weights -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)"
      ],
      "metadata": {
        "id": "_U7d8O-ItU9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(X, Y, method, optimizer_type=None, lr=0.0001, n_epochs=100, batch_size=32, epsilon=1e-9, beta1=0.9, beta2=0.999):\n",
        "    x_set = X.values\n",
        "    y_set = Y.values\n",
        "    n_samples, n_features = X.shape\n",
        "    weights = np.zeros(n_features)\n",
        "    b = 0\n",
        "\n",
        "    optimizer = Optimizer(weights, lr=lr, beta1=beta1, beta2=beta2, epsilon=epsilon) if optimizer_type else None\n",
        "\n",
        "    losses = []\n",
        "    for epoch in range(n_epochs):\n",
        "        loss = []\n",
        "\n",
        "        if method == \"Batch\":\n",
        "            # Batch Gradient Descent\n",
        "            dw, db = gradient(x_set, y_set, weights, b)\n",
        "            if optimizer_type == \"Momentum\":\n",
        "                optimizer.momentum_update(dw)\n",
        "            elif optimizer_type == \"RMSProp\":\n",
        "                optimizer.rmsprop_update(dw)\n",
        "            elif optimizer_type == \"Adam\":\n",
        "                optimizer.adam_update(dw)\n",
        "            else:\n",
        "                weights -= lr * dw\n",
        "            b -= lr * db\n",
        "\n",
        "            # Compute loss for the entire dataset\n",
        "            a = sigmoid(np.dot(x_set, weights) + b)\n",
        "            loss.append(compute_loss(y_set, a, epsilon))\n",
        "\n",
        "        elif method == \"Mini-Batch\":\n",
        "            # Mini-Batch Gradient Descent\n",
        "            indices = np.arange(n_samples)\n",
        "            np.random.shuffle(indices)\n",
        "            x_set = x_set[indices]\n",
        "            y_set = y_set[indices]\n",
        "\n",
        "            for i in range(0, n_samples, batch_size):\n",
        "                xi = x_set[i:i+batch_size]\n",
        "                yi = y_set[i:i+batch_size]\n",
        "\n",
        "                dw, db = gradient(xi, yi, weights, b)\n",
        "                if optimizer_type == \"Momentum\":\n",
        "                    optimizer.momentum_update(dw)\n",
        "                elif optimizer_type == \"RMSProp\":\n",
        "                    optimizer.rmsprop_update(dw)\n",
        "                elif optimizer_type == \"Adam\":\n",
        "                    optimizer.adam_update(dw)\n",
        "                else:\n",
        "                    weights -= lr * dw\n",
        "                b -= lr * db\n",
        "\n",
        "                # Compute loss for the batch\n",
        "                a = sigmoid(np.dot(xi, weights) + b)\n",
        "                loss.append(compute_loss(yi, a, epsilon))\n",
        "\n",
        "        elif method == \"SGD\":\n",
        "            # Stochastic Gradient Descent\n",
        "            indices = np.arange(n_samples)\n",
        "            np.random.shuffle(indices)\n",
        "\n",
        "            for i in indices:\n",
        "                xi = x_set[i:i+1]\n",
        "                yi = y_set[i:i+1]\n",
        "\n",
        "                dw, db = gradient(xi, yi, weights, b)\n",
        "                if optimizer_type == \"Momentum\":\n",
        "                    optimizer.momentum_update(dw)\n",
        "                elif optimizer_type == \"RMSProp\":\n",
        "                    optimizer.rmsprop_update(dw)\n",
        "                elif optimizer_type == \"Adam\":\n",
        "                    optimizer.adam_update(dw)\n",
        "                else:\n",
        "                    weights -= lr * dw\n",
        "                b -= lr * db\n",
        "\n",
        "                # Compute loss for the sample\n",
        "                a = sigmoid(np.dot(xi, weights) + b)\n",
        "                loss.append(compute_loss(yi, a, epsilon))\n",
        "\n",
        "        # Average loss for the epoch\n",
        "        losses.append(np.mean(loss))\n",
        "        # if epoch % 10 == 0:\n",
        "            # print(f\"Epoch {epoch}, Loss: {losses[-1]:.4f}\")\n",
        "\n",
        "    return weights, b, losses\n"
      ],
      "metadata": {
        "id": "QfXswyUDtWCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_encoded.copy().drop(['Conversion'], axis=1)\n",
        "Y = df_encoded.copy()['Conversion']\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
        "\n",
        "# precision, accuracy, recall, F1 score\n",
        "results = []\n",
        "\n",
        "weights, bias, losses = sgd(X_train, Y_train, method='SGD', optimizer_type='Momentum', n_epochs=100)\n",
        "y_predictor, y_score = get_ypred(X_test, weights, bias)\n",
        "results.append(evaluate_metrics(y_predictor, Y_test))\n",
        "weights, bias, losses = sgd(X_train, Y_train, method='SGD', optimizer_type='RSMProp', n_epochs=100)\n",
        "y_predictor, y_score = get_ypred(X_test, weights, bias)\n",
        "results.append(evaluate_metrics(y_predictor, Y_test))\n",
        "weights, bias, losses = sgd(X_train, Y_train, method='SGD', optimizer_type='Adam', n_epochs=100)\n",
        "y_predictor, y_score = get_ypred(X_test, weights, bias)\n",
        "results.append(evaluate_metrics(y_predictor, Y_test))\n",
        "weights, bias, losses = sgd(X_train, Y_train, method='Batch', optimizer_type='Momentum', n_epochs=100)\n",
        "y_predictor, y_score = get_ypred(X_test, weights, bias)\n",
        "results.append(evaluate_metrics(y_predictor, Y_test))\n",
        "weights, bias, losses = sgd(X_train, Y_train, method='Batch', optimizer_type='RSMProp', n_epochs=100)\n",
        "y_predictor, y_score = get_ypred(X_test, weights, bias)\n",
        "results.append(evaluate_metrics(y_predictor, Y_test))\n",
        "weights, bias, losses = sgd(X_train, Y_train, method='Batch', optimizer_type='Adam', n_epochs=100)\n",
        "y_predictor, y_score = get_ypred(X_test, weights, bias)\n",
        "results.append(evaluate_metrics(y_predictor, Y_test))\n",
        "weights, bias, losses = sgd(X_train, Y_train, method='Mini-Batch', optimizer_type='Momentum', n_epochs=100)\n",
        "y_predictor, y_score = get_ypred(X_test, weights, bias)\n",
        "results.append(evaluate_metrics(y_predictor, Y_test))\n",
        "weights, bias, losses = sgd(X_train, Y_train, method='Mini-Batch', optimizer_type='RSMProp', n_epochs=100)\n",
        "y_predictor, y_score = get_ypred(X_test, weights, bias)\n",
        "results.append(evaluate_metrics(y_predictor, Y_test))\n",
        "weights, bias, losses = sgd(X_train, Y_train, method='Mini-Batch', optimizer_type='Adam', n_epochs=100)\n",
        "y_predictor, y_score = get_ypred(X_test, weights, bias)\n",
        "results.append(evaluate_metrics(y_predictor, Y_test))\n",
        "\n",
        "for i in results:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "4wG5TLkVtXCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameter tuning\n",
        "\n",
        "def hyperparameter_tuning(X_train, Y_train, methods, optimizers, param_grid, n_epochs=100):\n",
        "    best_results = []\n",
        "\n",
        "    for method in methods:\n",
        "        for optimizer_type in optimizers:\n",
        "            # print(f\"Testing method: {method} with optimizer: {optimizer_type}\")\n",
        "\n",
        "            # Get parameter combinations for the current optimizer type\n",
        "            param_combinations = list(ParameterGrid(param_grid[optimizer_type]))\n",
        "\n",
        "            best_params = None\n",
        "            best_loss = float(\"inf\")\n",
        "\n",
        "            for params in param_combinations:\n",
        "                # print(f\"Testing parameters: {params}\")\n",
        "\n",
        "                # Run the optimizer on the current method\n",
        "                weights, bias, losses = sgd(\n",
        "                    X_train,\n",
        "                    Y_train,\n",
        "                    method=method,\n",
        "                    optimizer_type=optimizer_type,\n",
        "                    lr=params.get(\"lr\", 0.001),\n",
        "                    n_epochs=n_epochs,\n",
        "                    batch_size=params.get(\"batch_size\", 32),\n",
        "                    beta1=params.get(\"beta1\", 0.9),\n",
        "                    beta2=params.get(\"beta2\", 0.999),\n",
        "                )\n",
        "\n",
        "                final_loss = losses[-1]\n",
        "\n",
        "                # Track the best combination for the current method + optimizer\n",
        "                if final_loss < best_loss:\n",
        "                    best_loss = final_loss\n",
        "                    best_params = params\n",
        "\n",
        "                # print(f\"Final loss for parameters {params}: {final_loss:.4f}\")\n",
        "\n",
        "            # print(f\"Best parameters for method: {method} with optimizer: {optimizer_type} => {best_params}, Loss: {best_loss:.4f}\")\n",
        "            best_results.append({\n",
        "                \"method\": method,\n",
        "                \"optimizer\": optimizer_type,\n",
        "                \"best_params\": best_params,\n",
        "                \"best_loss\": best_loss\n",
        "            })\n",
        "\n",
        "    return best_results\n",
        "\n",
        "# Example usage\n",
        "methods = [\"SGD\", \"Batch\", \"Mini-Batch\"]\n",
        "optimizers = [\"Momentum\", \"RMSProp\", \"Adam\"]\n",
        "\n",
        "param_grid = {\n",
        "    \"Momentum\": {\"lr\": [0.01, 0.001], \"beta1\": [0.9, 0.95], \"batch_size\": [32, 64]},\n",
        "    \"RMSProp\": {\"lr\": [0.001, 0.0001], \"beta2\": [0.9, 0.99], \"batch_size\": [32, 64]},\n",
        "    \"Adam\": {\"lr\": [0.001, 0.0001], \"beta1\": [0.9, 0.95], \"beta2\": [0.99, 0.999], \"batch_size\": [32, 64]},\n",
        "}\n",
        "\n",
        "best_results = hyperparameter_tuning(X_train, Y_train, methods, optimizers, param_grid, n_epochs=10)\n",
        "\n",
        "results_df = pd.DataFrame(best_results)\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "bO2ap1jatYB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Conclude by discussing the practical trade-offs of the algorithms, including\n",
        "computational complexity, interpretability, and suitability for large-scale datasets."
      ],
      "metadata": {
        "id": "XOKPOjWhtyPe"
      }
    }
  ]
}