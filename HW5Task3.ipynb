{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ANS1514/Projects_in_AI-ML/blob/main/HW5Task3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parts 1 + 2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, Layer, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import random\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction, sentence_bleu\n",
        "# For part 3 + 4\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ],
      "metadata": {
        "id": "1AACJrgpbm84"
      },
      "id": "1AACJrgpbm84",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 1 (10 points): Implement the scaled dot-product attention as discussed in class\n",
        "(lecture 14) from scratch (use NumPy and pandas only, no deep learning libraries are\n",
        "allowed for this step)."
      ],
      "metadata": {
        "id": "G_FmFzTcbqhf"
      },
      "id": "G_FmFzTcbqhf"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "862808d1",
      "metadata": {
        "id": "862808d1"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    d_k = Q.shape[-1]\n",
        "    scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n",
        "    attention_weights = softmax(scores)\n",
        "    output = np.matmul(attention_weights, V)\n",
        "\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2 (10 points): Pick any encoder-decoder seq2seq model (as discussed in class) and\n",
        "integrate the scaled dot-product attention in the encoder architecture. You may come\n",
        "up with your own technique of integration or adopt one from literature. Hint: See\n",
        "Bahdanau or Luong attention paper presented in class (lecture 14)"
      ],
      "metadata": {
        "id": "-kCLEPUnbvR1"
      },
      "id": "-kCLEPUnbvR1"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a43f0533",
      "metadata": {
        "id": "a43f0533"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(ScaledDotProductAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Q, K, V = inputs\n",
        "        d_k = tf.cast(tf.shape(K)[-1], tf.float32)\n",
        "        # Scaled dot-product\n",
        "        scores = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(d_k)\n",
        "        # Apply softmax\n",
        "        attn_weights = tf.nn.softmax(scores, axis=-1)\n",
        "        output = tf.matmul(attn_weights, V)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 3 (5 points): Pick any public dataset of your choice (use a small-scale dataset like a\n",
        "subset of the Tatoeba or Multi30k dataset) for machine translation task. Train your\n",
        "model from Part 2 for the machine translation task. Evaluate test set by reporting the\n",
        "BLEU Score"
      ],
      "metadata": {
        "id": "xArUJ_FucB42"
      },
      "id": "xArUJ_FucB42"
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Processing\n",
        "nltk.download('punkt')\n",
        "\n",
        "data = pd.read_csv(\"/content/tatoeba.tsv\", sep=\"\\t\", header=None, quoting=3)\n",
        "data.columns = [\"id_src\", \"eng\", \"id_tgt\", \"de\"]\n",
        "\n",
        "data_shuffled = data.sample(frac=1, random_state=42)\n",
        "data_unique = data_shuffled.drop_duplicates(subset=[\"eng\"]).reset_index(drop=True)\n",
        "\n",
        "subset_size = 10000\n",
        "data_subset = data_unique.head(subset_size)\n",
        "print(\"Number of samples in our subset:\", len(data_subset))\n",
        "\n",
        "data_pairs = list(zip(data_subset[\"eng\"], data_subset[\"de\"]))\n",
        "\n",
        "# Shuffle and split\n",
        "random.shuffle(data_pairs)\n",
        "split_idx = int(0.8 * len(data_pairs))\n",
        "train_pairs = data_pairs[:split_idx]\n",
        "test_pairs = data_pairs[split_idx:]\n",
        "\n",
        "# Function to preprocess sentences\n",
        "def preprocess_sentence(sentence, is_target=False):\n",
        "    sentence = sentence.lower().strip()\n",
        "    if is_target:\n",
        "        sentence = '<start> ' + sentence + ' <end>'\n",
        "    return sentence\n",
        "\n",
        "# Preprocess data\n",
        "train_src = [preprocess_sentence(pair[0]) for pair in train_pairs]\n",
        "train_tgt = [preprocess_sentence(pair[1], is_target=True) for pair in train_pairs]\n",
        "test_src = [preprocess_sentence(pair[0]) for pair in test_pairs]\n",
        "test_tgt = [preprocess_sentence(pair[1], is_target=True) for pair in test_pairs]\n",
        "\n",
        "# Create tokenizers for source and target\n",
        "src_tokenizer = Tokenizer(filters='')\n",
        "src_tokenizer.fit_on_texts(train_src)\n",
        "tgt_tokenizer = Tokenizer(filters='')\n",
        "tgt_tokenizer.fit_on_texts(train_tgt)\n",
        "\n",
        "# Vocabulary sizes\n",
        "src_vocab_size = len(src_tokenizer.word_index) + 1\n",
        "tgt_vocab_size = len(tgt_tokenizer.word_index) + 1\n",
        "\n",
        "# Convert sentences to sequences\n",
        "train_src_seq = src_tokenizer.texts_to_sequences(train_src)\n",
        "train_tgt_seq = tgt_tokenizer.texts_to_sequences(train_tgt)\n",
        "test_src_seq = src_tokenizer.texts_to_sequences(test_src)\n",
        "test_tgt_seq = tgt_tokenizer.texts_to_sequences(test_tgt)\n",
        "\n",
        "# Pad sequences to the maximum length found in training\n",
        "max_src_length = max(len(seq) for seq in train_src_seq)\n",
        "max_tgt_length = max(len(seq) for seq in train_tgt_seq)\n",
        "\n",
        "train_src_seq = pad_sequences(train_src_seq, maxlen=max_src_length, padding='post')\n",
        "train_tgt_seq = pad_sequences(train_tgt_seq, maxlen=max_tgt_length, padding='post')\n",
        "test_src_seq = pad_sequences(test_src_seq, maxlen=max_src_length, padding='post')\n",
        "test_tgt_seq = pad_sequences(test_tgt_seq, maxlen=max_tgt_length, padding='post')\n",
        "\n",
        "train_decoder_input = train_tgt_seq[:, :-1]\n",
        "train_decoder_output = train_tgt_seq[:, 1:]\n",
        "test_decoder_input = test_tgt_seq[:, :-1]\n",
        "test_decoder_output = test_tgt_seq[:, 1:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibxcV_2xcgL-",
        "outputId": "fa7a9787-deb6-4ff0-c43b-67d34608e4a8"
      },
      "id": "ibxcV_2xcgL-",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in our subset: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "embedding_dim = 64\n",
        "lstm_units = 64\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_src_length,), name='encoder_inputs')\n",
        "encoder_embedding = Embedding(src_vocab_size, embedding_dim, mask_zero=False, name='encoder_embedding')(encoder_inputs) # Change mask_zero to False\n",
        "encoder_lstm, state_h, state_c = LSTM(lstm_units, return_sequences=True, return_state=True, name='encoder_lstm', use_cudnn=False)(encoder_embedding) # Disable cuDNN\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_tgt_length - 1,), name='decoder_inputs')\n",
        "decoder_embedding = Embedding(tgt_vocab_size, embedding_dim, mask_zero=False, name='decoder_embedding')(decoder_inputs) # Change mask_zero to False\n",
        "decoder_lstm, _, _ = LSTM(lstm_units, return_sequences=True, return_state=True, name='decoder_lstm', use_cudnn=False)(\n",
        "    decoder_embedding, initial_state=encoder_states)\n",
        "\n",
        "# ... rest of your code ...\n",
        "# Apply attention\n",
        "attn_out = ScaledDotProductAttention(name='attention')([decoder_lstm, encoder_lstm, encoder_lstm])\n",
        "decoder_combined_context = Concatenate(axis=-1, name='concat_layer')([decoder_lstm, attn_out])\n",
        "\n",
        "# Final dense layer for predicting target tokens\n",
        "decoder_dense = Dense(tgt_vocab_size, activation='softmax', name='decoder_dense')\n",
        "decoder_outputs = decoder_dense(decoder_combined_context)\n",
        "\n",
        "# Define the training model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"Model summary:\")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "M2RjAXT5cr5l",
        "outputId": "13164509-977a-44e5-870c-af9bb85a9663"
      },
      "id": "M2RjAXT5cr5l",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m71\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_inputs            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m71\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m550,208\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m713,664\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m71\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │         \u001b[38;5;34m33,024\u001b[0m │ encoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,     │                │                        │\n",
              "│                           │ \u001b[38;5;34m64\u001b[0m)]                   │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │         \u001b[38;5;34m33,024\u001b[0m │ decoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,     │                │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],    │\n",
              "│                           │ \u001b[38;5;34m64\u001b[0m)]                   │                │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attention                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│ (\u001b[38;5;33mScaledDotProductAttenti…\u001b[0m │                        │                │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│                           │                        │                │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concat_layer              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_dense (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m, \u001b[38;5;34m11151\u001b[0m)      │      \u001b[38;5;34m1,438,479\u001b[0m │ concat_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_inputs            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">550,208</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">713,664</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ encoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │                │                        │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)]                   │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ decoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │                │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],    │\n",
              "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)]                   │                │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ attention                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ScaledDotProductAttenti…</span> │                        │                │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│                           │                        │                │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concat_layer              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11151</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,438,479</span> │ concat_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,768,399\u001b[0m (10.56 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,768,399</span> (10.56 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,768,399\u001b[0m (10.56 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,768,399</span> (10.56 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 3 (5 points): Pick any public dataset of your choice (use a small-scale dataset like a\n",
        "subset of the Tatoeba or Multi30k dataset) for machine translation task. Train your\n",
        "model from Part 2 for the machine translation task. Evaluate test set by reporting the\n",
        "BLEU Score"
      ],
      "metadata": {
        "id": "hFK8SX6ZrjJG"
      },
      "id": "hFK8SX6ZrjJG"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8b3b53ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b3b53ac",
        "outputId": "1472a011-f8c9-459b-ce72-5cb509dadcf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - accuracy: 0.8562 - loss: 3.8103 - val_accuracy: 0.8982 - val_loss: 0.8034\n",
            "Epoch 2/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.8982 - loss: 0.7832 - val_accuracy: 0.8994 - val_loss: 0.7878\n",
            "Epoch 3/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.8984 - loss: 0.7606 - val_accuracy: 0.9001 - val_loss: 0.7756\n",
            "Epoch 4/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.8998 - loss: 0.7310 - val_accuracy: 0.9007 - val_loss: 0.7688\n",
            "Epoch 5/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9017 - loss: 0.7074 - val_accuracy: 0.9023 - val_loss: 0.7612\n",
            "Epoch 6/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9017 - loss: 0.6998 - val_accuracy: 0.9027 - val_loss: 0.7557\n",
            "Epoch 7/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9028 - loss: 0.6783 - val_accuracy: 0.9034 - val_loss: 0.7503\n",
            "Epoch 8/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9035 - loss: 0.6659 - val_accuracy: 0.9044 - val_loss: 0.7403\n",
            "Epoch 9/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9042 - loss: 0.6455 - val_accuracy: 0.9047 - val_loss: 0.7361\n",
            "Epoch 10/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9066 - loss: 0.6187 - val_accuracy: 0.9051 - val_loss: 0.7326\n",
            "Epoch 11/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9063 - loss: 0.6124 - val_accuracy: 0.9055 - val_loss: 0.7299\n",
            "Epoch 12/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9062 - loss: 0.6055 - val_accuracy: 0.9060 - val_loss: 0.7280\n",
            "Epoch 13/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9078 - loss: 0.5851 - val_accuracy: 0.9060 - val_loss: 0.7277\n",
            "Epoch 14/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9097 - loss: 0.5619 - val_accuracy: 0.9065 - val_loss: 0.7263\n",
            "Epoch 15/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9113 - loss: 0.5467 - val_accuracy: 0.9074 - val_loss: 0.7230\n",
            "Epoch 16/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9110 - loss: 0.5399 - val_accuracy: 0.9080 - val_loss: 0.7231\n",
            "Epoch 17/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9134 - loss: 0.5153 - val_accuracy: 0.9071 - val_loss: 0.7247\n",
            "Epoch 18/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9139 - loss: 0.5056 - val_accuracy: 0.9082 - val_loss: 0.7216\n",
            "Epoch 19/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9161 - loss: 0.4836 - val_accuracy: 0.9085 - val_loss: 0.7222\n",
            "Epoch 20/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9164 - loss: 0.4730 - val_accuracy: 0.9088 - val_loss: 0.7234\n",
            "Epoch 21/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9180 - loss: 0.4575 - val_accuracy: 0.9084 - val_loss: 0.7249\n",
            "Epoch 22/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9192 - loss: 0.4416 - val_accuracy: 0.9090 - val_loss: 0.7247\n",
            "Epoch 23/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9202 - loss: 0.4308 - val_accuracy: 0.9083 - val_loss: 0.7316\n",
            "Epoch 24/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9219 - loss: 0.4178 - val_accuracy: 0.9093 - val_loss: 0.7288\n",
            "Epoch 25/25\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9233 - loss: 0.4047 - val_accuracy: 0.9083 - val_loss: 0.7364\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 109ms/step\n",
            "\n",
            "BLEU score on test set: 0.012911964775959716\n",
            "\n",
            "Examples on test set:\n",
            "Source: tom announced that he was leaving the\n",
            "Reference: <start> tom kündigte an, die gruppe verlassen zu wollen.\n",
            "Prediction: tom war und dass ganze hat\n",
            "------\n",
            "Source: he has something i confidence.\n",
            "Reference: <start> er hat etwas, was ich nicht habe -\n",
            "Prediction: ich war mir das ich mich ist. er er\n",
            "------\n",
            "Source: you look very nice.\n",
            "Reference: <start> du siehst sehr gut aus.\n",
            "Prediction: du bist das sehr zu\n",
            "------\n",
            "Source: i know that you're not ready to deal with this.\n",
            "Reference: <start> ich weiß, daß du nicht bereit bist, dich damit zu\n",
            "Prediction: ich werde dass sie das so so was zu zu tun.\n",
            "------\n",
            "Source: why do you always want to\n",
            "Reference: <start> warum willst du immer\n",
            "Prediction: hast hast du tom tom\n",
            "------\n",
            "Source: i don't think tom was wrong.\n",
            "Reference: <start> ich denke nicht, dass tom\n",
            "Prediction: ich habe nicht, dass ich nicht\n",
            "------\n",
            "Source: mary was working her\n",
            "Reference: <start> maria ihr land.\n",
            "Prediction: er hat hat\n",
            "------\n",
            "Source: if he makes that face it's because he's nervous.\n",
            "Reference: <start> wenn er dieses gesicht macht, dann liegt das daran, dass er nervös ist.\n",
            "Prediction: er er mich bahnhof sich er war in in er er uns sei.\n",
            "------\n",
            "Source: i love it when they leave the open.\n",
            "Reference: <start> ich mag es, wenn sie die offen lassen.\n",
            "Prediction: ich habe es dass ich das tür der\n",
            "------\n",
            "Source: tom is busy now, isn't he?\n",
            "Reference: <start> tom ist jetzt beschäftigt, oder?\n",
            "Prediction: tom ist nicht nicht als\n",
            "------\n"
          ]
        }
      ],
      "source": [
        "train_decoder_output = np.expand_dims(train_decoder_output, -1)\n",
        "test_decoder_output = np.expand_dims(test_decoder_output, -1)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit([train_src_seq, train_decoder_input], train_decoder_output,\n",
        "                    batch_size=32,\n",
        "                    epochs=25,\n",
        "                    validation_split=0.2)\n",
        "\n",
        "\n",
        "predictions = model.predict([test_src_seq, test_decoder_input])\n",
        "predicted_sequences = np.argmax(predictions, axis=-1)\n",
        "\n",
        "# Create inverse mapping\n",
        "src_index_word = {v: k for k, v in src_tokenizer.word_index.items()}\n",
        "tgt_index_word = {v: k for k, v in tgt_tokenizer.word_index.items()}\n",
        "\n",
        "def sequence_to_text(seq, tokenizer_index_word):\n",
        "    words = []\n",
        "    for idx in seq:\n",
        "        if idx == 0:\n",
        "            continue\n",
        "        word = tokenizer_index_word.get(idx, '')\n",
        "        if word == '<end>':\n",
        "            break\n",
        "        words.append(word)\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Decode predictions and references\n",
        "predicted_sentences = [sequence_to_text(seq, tgt_index_word) for seq in predicted_sequences]\n",
        "reference_sentences = [sequence_to_text(seq, tgt_index_word) for seq in test_tgt_seq]\n",
        "\n",
        "# Prepare lists for corpus_bleu\n",
        "reference_list = [[ref.split()] for ref in reference_sentences]\n",
        "hypothesis_list = [pred.split() for pred in predicted_sentences]\n",
        "\n",
        "smoothing_fn = SmoothingFunction().method1\n",
        "bleu_score = corpus_bleu(reference_list, hypothesis_list, smoothing_function=smoothing_fn)\n",
        "print(\"\\nBLEU score on test set:\", bleu_score)\n",
        "\n",
        "print(\"\\nExamples on test set:\")\n",
        "for i in range(len(test_src_seq)):\n",
        "  if i < 10:\n",
        "    print(\"Source:\", sequence_to_text(test_src_seq[i], src_index_word))\n",
        "    print(\"Reference:\", sequence_to_text(test_tgt_seq[i], tgt_index_word))\n",
        "    print(\"Prediction:\", predicted_sentences[i])\n",
        "    print(\"------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 4 (30 points): In this part you are required to implement a simplified Transformer\n",
        "model from scratch (using Python and NumPy/PyTorch/TensorFlow with minimal highlevel abstractions) and apply it to a machine translation task (e.g., English-to-French or\n",
        "English-to-German translation) using the same dataset from part 3.\n"
      ],
      "metadata": {
        "id": "-LQ3_LRlrx74"
      },
      "id": "-LQ3_LRlrx74"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bba811e9",
      "metadata": {
        "id": "bba811e9"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.pe[:, :seq_len, :]\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7ab872f",
      "metadata": {
        "id": "e7ab872f"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model=64, num_heads=2):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.linear_Q = nn.Linear(d_model, d_model)\n",
        "        self.linear_K = nn.Linear(d_model, d_model)\n",
        "        self.linear_V = nn.Linear(d_model, d_model)\n",
        "        self.out_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        batch_size = Q.size(0)\n",
        "\n",
        "        # Linear projections\n",
        "        Q = self.linear_Q(Q)\n",
        "        K = self.linear_K(K)\n",
        "        V = self.linear_V(V)\n",
        "\n",
        "        # Split into multiple heads and transpose\n",
        "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
        "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
        "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n",
        "\n",
        "        # Scaled dot-product attention for each head\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_weights = torch.softmax(scores, dim=-1)\n",
        "        attn_output = torch.matmul(attn_weights, V)\n",
        "\n",
        "        # Concatenate heads\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
        "        output = self.out_linear(attn_output)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56a630bf",
      "metadata": {
        "id": "56a630bf"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=64, num_heads=2, d_ff=128, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.layernorm1 = nn.LayerNorm(d_model)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Multi-head attention sublayer\n",
        "        attn_output = self.mha(x, x, x, mask)\n",
        "        x = self.layernorm1(x + self.dropout(attn_output))\n",
        "\n",
        "        # Feedforward network\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.layernorm2(x + self.dropout(ffn_output))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22d0ec13",
      "metadata": {
        "id": "22d0ec13"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model=64, num_heads=2, d_ff=128, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.layernorm1 = nn.LayerNorm(d_model)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model)\n",
        "        self.layernorm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
        "        # Masked self-attention\n",
        "        attn1 = self.mha1(x, x, x, mask=tgt_mask)\n",
        "        x = self.layernorm1(x + self.dropout(attn1))\n",
        "\n",
        "        # Encoder-decoder attention\n",
        "        attn2 = self.mha2(x, enc_output, enc_output, mask=src_mask)\n",
        "        x = self.layernorm2(x + self.dropout(attn2))\n",
        "\n",
        "        # Feedforward\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.layernorm3(x + self.dropout(ffn_output))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3b8fa9c",
      "metadata": {
        "id": "b3b8fa9c"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=64, num_heads=2, d_ff=128, num_encoder_layers=2, num_decoder_layers=2, dropout=0.1, max_len=100):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "        self.pos_decoder = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        # Encoder layers\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_encoder_layers)])\n",
        "\n",
        "        # Decoder layers\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_decoder_layers)])\n",
        "\n",
        "        self.final_linear = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        # Mask to ensure that position i can only attend to positions ≤ i\n",
        "        mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
        "        return mask.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # src, tgt: (batch_size, seq_len)\n",
        "        src_mask = None\n",
        "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
        "\n",
        "        # Embedding and positional encoding\n",
        "        src = self.pos_encoder(self.src_embedding(src))\n",
        "        tgt = self.pos_decoder(self.tgt_embedding(tgt))\n",
        "\n",
        "        # Encoder forward\n",
        "        for layer in self.encoder_layers:\n",
        "            src = layer(src, mask=src_mask)\n",
        "        enc_output = src\n",
        "\n",
        "        # Decoder forward\n",
        "        x = tgt\n",
        "        for layer in self.decoder_layers:\n",
        "            x = layer(x, enc_output, src_mask=src_mask, tgt_mask=tgt_mask)\n",
        "\n",
        "        # Final linear layer and softmax\n",
        "        output = self.final_linear(x)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3e6da59",
      "metadata": {
        "id": "b3e6da59"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_seqs, tgt_seqs):\n",
        "        self.src_seqs = src_seqs\n",
        "        self.tgt_seqs = tgt_seqs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_seqs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = torch.tensor(self.src_seqs[idx], dtype=torch.long)\n",
        "        tgt = torch.tensor(self.tgt_seqs[idx], dtype=torch.long)\n",
        "        tgt_input = tgt[:-1]\n",
        "        tgt_output = tgt[1:]\n",
        "        return src, tgt_input, tgt_output\n",
        "\n",
        "# Training and test datasets.\n",
        "train_dataset = TranslationDataset(train_src_seq, train_tgt_seq)\n",
        "test_dataset = TranslationDataset(test_src_seq, test_tgt_seq)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cbcc923",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cbcc923",
        "outputId": "2afedb6a-f135-450b-a256-cedb4cd579cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using max_len_model: 71\n"
          ]
        }
      ],
      "source": [
        "max_len_model = max(max_src_length, max_tgt_length)\n",
        "print(\"Using max_len_model:\", max_len_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d17b587",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d17b587",
        "outputId": "9a97c44a-7389-43ae-8043-7246700b7bce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 6.9957\n",
            "Epoch 2 Loss: 5.0911\n",
            "Epoch 3 Loss: 3.7838\n",
            "Epoch 4 Loss: 2.9185\n",
            "Epoch 5 Loss: 2.2848\n",
            "Epoch 6 Loss: 1.8330\n",
            "Epoch 7 Loss: 1.6045\n",
            "Epoch 8 Loss: 1.4098\n",
            "Epoch 9 Loss: 1.2305\n",
            "Epoch 10 Loss: 1.0687\n",
            "Epoch 11 Loss: 0.9275\n",
            "Epoch 12 Loss: 0.8563\n",
            "Epoch 13 Loss: 0.7872\n",
            "Epoch 14 Loss: 0.7258\n",
            "Epoch 15 Loss: 0.6656\n",
            "Epoch 16 Loss: 0.6096\n",
            "Epoch 17 Loss: 0.5803\n",
            "Epoch 18 Loss: 0.5547\n",
            "Epoch 19 Loss: 0.5271\n",
            "Epoch 20 Loss: 0.5020\n",
            "Epoch 21 Loss: 0.4758\n",
            "Epoch 22 Loss: 0.4636\n",
            "Epoch 23 Loss: 0.4516\n",
            "Epoch 24 Loss: 0.4386\n",
            "Epoch 25 Loss: 0.4273\n",
            "Average BLEU score on test set: 0.0000\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model=64, num_heads=2, d_ff=128,\n",
        "                    num_encoder_layers=2, num_decoder_layers=2, dropout=0.1, max_len=max_len_model).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "# training loop\n",
        "num_epochs = 25\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for src, tgt_input, tgt_output in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        src = src.to(device)\n",
        "        tgt_input = tgt_input.to(device)\n",
        "        tgt_output = tgt_output.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(src, tgt_input)\n",
        "        # Reshape outputs\n",
        "        output = output.view(-1, tgt_vocab_size)\n",
        "        tgt_output = tgt_output.view(-1)\n",
        "\n",
        "        loss = criterion(output, tgt_output)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch+1} Loss: {epoch_loss/len(train_loader):.4f}\")\n",
        "\n",
        "\n",
        "model.eval()\n",
        "smoothing_fn = SmoothingFunction().method1\n",
        "bleu_scores = []\n",
        "for src, tgt_input, tgt_output in test_loader:\n",
        "    src = src.to(device)\n",
        "    tgt_input = tgt_input.to(device)\n",
        "    tgt_output = tgt_output.to(device)\n",
        "\n",
        "    decoder_input = tgt_input[:, :1]\n",
        "    max_len = max_tgt_length - 1\n",
        "    for i in range(max_len):\n",
        "        output = model(src, decoder_input)\n",
        "        next_token = output[:, -1, :].argmax(dim=-1, keepdim=True)\n",
        "        decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
        "        if (next_token == 2).all():\n",
        "            break\n",
        "\n",
        "    ref = tgt_output.squeeze().tolist()\n",
        "    hyp = decoder_input.squeeze().tolist()\n",
        "\n",
        "    ref = [str(x) for x in ref]\n",
        "    hyp = [str(x) for x in hyp]\n",
        "\n",
        "    bleu = sentence_bleu([ref], hyp, smoothing_function=smoothing_fn)\n",
        "    bleu_scores.append(bleu)\n",
        "\n",
        "print(\"Average BLEU score on test set: {:.4f}\".format(sum(bleu_scores)/len(bleu_scores)))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}